\documentclass[12pt,a4paper]{article}
\usepackage[style = authoryear, maxcitenames = 1, uniquelist=false, sorting = nyt, abbreviate = true, doi = true, backend = biber]{biblatex}
\usepackage[lmargin = 3.5cm, rmargin = 3.5cm, tmargin = 2.5cm, bmargin = 2.5cm]{geometry}
\usepackage[onehalfspacing]{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{times}
\usepackage{csquotes}
\usepackage[UKenglish]{babel}
\usepackage[textsize=tiny]{todonotes}
\usepackage[acronym]{glossaries}
\usepackage{soul} % for command \hl
\usepackage{pdfpages} % to insert pdf docs
\usepackage{csvsimple} % to import .csv files as tables
\usepackage{siunitx}
\usepackage{tikz}
\usepackage{caption} 
\usepackage{float}
\captionsetup[table]{skip=10pt} % more space between table and caption

\AtBeginBibliography{\small}
\addbibresource{main_sources.bib}

\setlength{\parindent}{0cm}
\setlength{\marginparwidth}{3.5cm} % make todonotes wider
\newcommand{\todoleft}[1]{{\reversemarginpar \todo{#1}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\def\findate{\today}


%title page
\thispagestyle{empty}
\begin{center}
    \Large{University of Innsbruck \\ Faculty of Mathematics, Computer Science and Physics} \\
    \vspace{3mm}
    \large{Institute for Theoretical Physics}
    \vspace{10mm}

    \includegraphics[width = 0.6 \linewidth]{logo.jpg}

    \vspace{10mm}
    \Large{Bachelor Thesis} \\
    \large{submitted for the degree of} \\
    \Large{Bachelor of Science} \\
    \vspace{10mm}
    \LARGE{\textbf{An information-theoretical approach to internal models in a Partially Observable Markov Decision Process}} \\
    \vspace{10mm}

    \large{by \\ Lukas Prader \\ Matriculation Nr.: 12115058 \\ SE Seminar with Bachelor Thesis}
\end{center}

\vspace{30mm}
\begin{tabular}{ll}
    \large{Submission Date:} & \large{\findate}                          \\
    \large{Supervisors:}     & \large{Alexander Vining, Hans J  Briegel} \\
\end{tabular}


\newpage
\thispagestyle{empty}
\begin{abstract}
    Lorem ipsum
\end{abstract}

\pagenumbering{roman}

\newpage
\tableofcontents
\thispagestyle{empty}
\newpage
\pagenumbering{arabic}

\section{Introduction} \label{sec:introduction}

A large reason for success in many, if not all scientific disciplines has been the adoption of a reductionist perspective on phenomena.
Widely adopted and successful, this hypothesis assumes that all processes in our universe are in the end governed by a set of fundamental laws, which can be used do describe any higher order of phenomena as well.
Especially in physics, the idea of a final, unified theory of everything has been seen as the ultimate goal of the discipline for centuries.

Yet, in many disciplines it has been shown that there are so-called emergent phenomena, which are not easily explained by lower-level fundamental laws, requiring additional concepts to accurately explain them \autocite{anderson1972more}.
Exactly how these complex phenomena can emerge from the set of currently known fundamental laws is an ongoing field of research, with interdisciplinary approaches taking from the fields of physics, chemistry, biology, psychology, philosophy computer science and others.

Especially the rise of artificial intelligence in recent years has produced new research trying to create complex models able to perform intelligent tasks.
Research into  so-called complex adaptive systems is also connected to research in biology, trying to understand the emergence of intelligent and adaptive behaviour in biological organisms.
Gaining insight into the mechanisms which enable biological systems to exhibit complex behaviour can in turn be used to improve the ways in which we attempt to create systems capable of these behaviours ourselves.

Complex system research can thus provide valuable insights into the emergence of intelligent behaviour, which also includes processes connected to adaptive behaviour and learning in animals.
Works by the likes of Pavlov and Skinner have probed into the mechanisms that influence behavioural patterns in animals \autocite{pavlov1906scientific, skinner1957experimental}.
Exactly how conditionable behaviour like this can emerge just from interaction with the environment is still not fully understood.

Information theory has been fundamental in furthering our understanding of such complex processes.
With the work of Shannon \autocite{shannon1948} as a basis, modern information theory has enabled researchers to quantify correlations and changes in complexity of dynamic systems.
It has successfully been applied to many examples in behavioural biology, such as looking into the collective behaviour of ants, information exchange in slime molds and group behaviour in bat populations \autocite{kim2021informational}.

Information theory can also be used to examine how biological agents, such as animals, acquire an understanding of their surroundings in order to then act in response, a so-called internal model of their environment.
Current approaches to create agents able to learn certain behaviours rely on trial and error to find the amount of parameters necessary to explain the complexity of a given environment, especially if only limited information about an environment is available to the agent.
Real life organisms do not seem to be limited as much, still being able to infer information about their environment even with limited information to their disposal.

Crutchfield and Feldman have proposed information theoretical quantities able to explain processes of inference for complex systems \autocite{crutchfield2003regularities}.
They can quantify the amount of "synchronisation" a system has achieved in comparison to a different system (like an environment), which it can interact with.
This framework can provide tools which may enable an agent to autonomously modify the current internal model in order to more accurately reflect the processes of the observed environment.

This thesis aims to apply some of the quantities proposed by Crutchfield and Feldman to a simple example of an agent acting with partial information of its environment.
The goal is to show the capabilities of this framework to enable evaluation of the current internal model and subsequent improvements to accurately reflect the environment, even with only partial information available to the agent.

% get paragraph about MDPs somewhere?

\newpage
\section{Methods} \label{sec:methods}

We will look at a small system related to behavioural biology, which can be modelled as a Markov Decision Process.
A Markov Decision Process (MDP) \autocite{bellman1957MDP} is defined by a set of states $S$, which are connected by transition probabilities, and a set of actions that an agent can perform in a given state.
These actions are determined by the agent's policy, commonly denoted with $\pi$.
One usually imposes the Markovian property, which implies that the transition probability from one state to the next only depends on the state itself and not the previous states of the system \autocite{cover1999elements}.

\subsection{The delayed action task} \label{ssec:delayed_action_mdp}
The system we will analyse is a delayed action task, which we want our agent to learn.
One can imagine a rat in a box, very similar to the aforementioned experiments by Skinner.
In this box, the rat observes a light switching on and off and has access to a button it can press.
The rat can obtain rewards, such as food, based on its actions. The light's state (on or off) changes in discrete time steps depending on the rat's actions.
In our setup, the rat receives a reward only if it presses the button at the correct time after the light turns on, specifically in the second time step.
This process can be visualized as the graph of an MDP, using three states and the possible transition probabilities as edges between them (Fig. \ref{fig:delayed_mdp}).

\begin{figure}[H]
    \centering
    % delayed action mdp
    \resizebox{0.5\linewidth}{!}{
        \begin{tikzpicture}[->,shorten >=1pt,auto,node distance=3cm,
                thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

            \node[fill=gray!50, circle, draw=black] (0) at (0,0) {$S_0$};
            \node[fill=cyan!50, circle, draw=black] (1) at (-1.5,-3) {$S_1$};
            \node[fill=cyan!50, circle, draw=black] (2) at (2,-1.5) {$S_2$};

            \path[every node/.style={font=\sffamily\scriptsize}]
            (0) edge [in=175,out=115, loop] node[above=3pt] {$(1-p(x)) \pi(a_0)$} (0)
            (0) edge [in=65,out=5, loop] node[above=3pt] {$(1-p(x)) \pi(w_0)$} (0)
            (0) edge [in=120,out=180] node[above, rotate=62] {$p(x) \pi(a_0)$} (1)
            (0) edge [bend right] node[below, rotate=62] {$p(x) \pi(w_0)$} (1)
            (1) edge [bend right] node[above, rotate=62] {$\pi(a_1)$} (0)
            (1) edge [bend right] node[rotate=18] {$\pi(w_1)$} (2)
            (2) edge [bend left] node[above, rotate=-42] {$\pi(w_2)$} (0)
            (2) edge [bend right, black!50!green] node[above, rotate=-42] {$\pi(a_2)$} (0);

        \end{tikzpicture}
    }
    \caption{\label{fig:delayed_mdp} Visualisation of the delayed action MDP. Colours of the states describe the state of the light (grey = dark, blue = light) Probabilities coming from the agent policy, acting $a_i$ or waiting $w_i$ for a state with index $i$, are denoted with $\pi$. The probability of the light turning on in the dark state is given as $p(x)$.}
\end{figure}

In the most general case, the policy of the agent can be different in every state $S_i$.
We will define the policy to be a probability, with probabilities of choosing to either wait $\pi(w_i)$ or act and press the button $\pi(a_i)$. One can specify the policy only by defining the probability to act, consequently choosing the probability to wait as the inverse probability $1-\pi(a_i)$.
We will assume that the state of the light is fully determined by the agents actions when being on, but if it is off, there is a probability of $p(x)$ for the light to turn on, independently of the agents action taken in the dark state.

The given MDP has two main properties, stationarity and irreducibility.
Being irreducible means that every state is reachable from every other state with positive probability in a finite number of steps \autocite{cover1999elements}, while stationarity implies that the transition probabilities do not change over time.
This is the case for our MDP, if we assume the policy to be fixed.

If we are specifically interested in the transitions for the states of the light, we can write the state transitions of this MDP into a state transition matrix $P$ (assuming some fixed policy $\pi$):
\begin{equation}
    \label{eq:P_mdp}
    P_{MDP} =
    \begin{bmatrix} 1 - p(x) & p(x) & 0            \\
                \pi(a_1) & 0    & 1 - \pi(a_1) \\
                1        & 0    & 0
    \end{bmatrix},
\end{equation}
with each row summing up to one.

One can see that the system reflected by this transition matrix does not capture the whole information about our initial system any more, ignoring the reward and the actions that do not directly influence the transitions of the light.

With the state transition matrix we can calculate the probability of ending up in a state in the next time step, given the probability of being in any of the states in the step before.
If we let the system transition for many time steps, the state distribution will converge to the so-called stationary distribution \autocite{cover1999elements}.
This stationary distribution $\mu$, a row vector by convention, satisfies the following equation:
\begin{equation}
    \label{eq:stationary_dist}
    \mu = P \mu.
\end{equation}

This means that $\mu$ can be calculated by finding the matrix eigenvector with eigenvalue 1.
The eigenvector should be re-scaled if necessary, such that the stationary distribution also has a sum of 1.

\subsection{Entropy and entropy rate} \label{ssec:entropy_rate}
Since we are particularly interested in the perspective of an agent observing this system, we will look at it as a process generating observations.
The agent can observe parameters of the system, in particular the current state of the light and the action it has just taken.
Over multiple time steps, these observations will form sequences made up of symbols, which correspond to the particular state that was observed.
These sequences will have a symbol distribution dependent on the nature of the generating process, motivating the use of information theory to analyse the properties of these sequences.

The standard Shannon entropy,
\begin{equation}
    \label{eq:shannon_entropy}
    H = -\sum_{x \epsilon \mathcal{X}} p(x) \log p(x),
\end{equation}
is defined with the sum over all symbols $x$ in a given alphabet $\mathcal{X}$, $\log$ meaning the binary logarithm here, as well as in the rest of this thesis.
Given a sequence, we can also calculate the entropy of tuples of symbols.
We can define the block entropy $H(s^L)$  as the entropy of "blocks" of symbols with length $L$:
\begin{equation}
    \label{eq:block_entropy}
    H(s^L) = -\sum_{s_i^L \epsilon S^L} p(s_i^L) \log p(s_i^L),
\end{equation}
looking at all possible blocks $s_i^L$ of length $L$, given a set of symbols $S$.
One simple intuition to understand this is to think about the entropy of words.
Instead of calculating the entropy of the individual letters, we calculate the entropy of length $L$ words in a given sentence.

The change of this block entropy for increasing block lengths is called the entropy rate $h_\mu$.
For an infinitely long sequence, the change in entropy converges to the final entropy rate \autocite{crutchfield2003regularities}:
\begin{equation}
    \label{eq:entropy_rate_limit}
    h_\mu = \lim_{L \to \infty} \frac{H(s^L)}{L}.
\end{equation}

This entropy rate can be interpreted as the inherent randomness of sequences obtained from the generating process \autocite{crutchfield2003regularities}.

For a stationary Markov Process, the entropy rate can be calculated if the stationary distribution and the transition matrix are known \autocite{cover1999elements}:
\begin{equation}
    \label{eq:entropy_rate_MP}
    h_\mu = -\sum_i \mu_i \sum_j P_{ij} \log P_{ij}.
\end{equation}

\subsection{Synchronisation and predictability gain} \label{ssec:synch_predgain}
Knowing how exactly the entropy rate of a system converges to its final value provides information about the complexity and structure of the system.
For a given block length, one can define the change of entropy at this length using the discrete derivative:
\begin{equation}
    \label{eq:entropy_change}
    \Delta H(s^L) = H(s^L) - H(s^{L-1}).
\end{equation}

This change in block entropy is equivalent to the estimated entropy rate $\hat{h}_\mu(L)$ at length $L$, which measures how random the system appears if only blocks up to length $L$ are observed.
It can be shown that $\Delta H(s^L)$ decreases monotonically for increasing $L$, implying that a finite estimate of $h_\mu$ will tend to overestimate the randomness of an incoming sequence, and thus of the generating process \autocite{crutchfield2003regularities}.

This opens the question of when an agent observing a sequence from a generating process can be said to have obtained all the information about the system.
At this point the agent would have all the information necessary to understand the nature of the generating process.
So-called "synchronisation" is achieved once the finite change in entropy is equal to the true final entropy rate of the system:
\begin{equation}
    \label{eq:synchronisation_equality}
    h_\mu - \Delta H(s^L) = 0.
\end{equation}
It means that there is no new information in blocks larger than $L$, for which synchronisation was achieved.

This criterion depends on knowing the true entropy rate of the generating process, or at least having a very good estimate of it.
In general, this might not be feasible to obtain for an agent, especially since the agent will have no prior knowledge about the generating process, which it could use to estimate the entropy rate.

A different, although weaker condition to characterize synchronisation is the fact that the entropy rate has to be constant from then on. This means that the second derivative $\Delta^2 H(s^L)$ will be 0.
We define
\begin{equation}
    \label{eq:predictability_gain}
    \Delta^2 H(s^L) = \Delta H(s^L) - \Delta H(s^{L-1}),
\end{equation}
also called predictability gain, which quantifies how much randomness is lost when using information of length $L$ blocks \autocite{crutchfield2003regularities}.
In order to sensibly define the predictability gain for $L=1$, one defines $\Delta H(s^0) = \log |S|$, using the number of symbols in $S$ defined by the cardinality $|S|$.
This is motivated by the idea that the randomness of the system is assumed to be maximal, if no sequences have yet been observed \autocite{crutchfield2003regularities}.
It is important to note that for synchronisation to be achieved, the predictability gain has to be zero for all following block lengths.
In some cases like periodic processes, it can happen that $\Delta^2 H(s^L)$ will be zero for some blocks, but different from zero for following blocks due to the periodic nature of the process \autocite{crutchfield2003regularities}.

\subsection{Observing the delayed action task} \label{ssec:observing_mdp}
In the case of the delayed action task (section \ref{ssec:delayed_action_mdp}), the ability for an agent to learn this task heavily depends on the type of information it has about the system.
If the agent is able to observe the true labels of each state, namely $S_0, S_1$ and $S_2$ (Fig. \ref{fig:obs_L_mdp}), it can easily find the optimal policy maximising the reward.

\begin{figure}[H]
    \centering
    % delayed action mdp
    \resizebox{0.7\linewidth}{!}{
        \begin{tikzpicture}[->,shorten >=1pt,auto,node distance=3cm,
                thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

            \node[fill=gray!50, circle, draw=black] (0) at (0,0) {$S_0$};
            \node[fill=cyan!50, circle, draw=black] (1) at (-1.5,-3) {$S_1$};
            \node[fill=cyan!50, circle, draw=black] (2) at (2,-1.5) {$S_2$};
            \node[fill=gray!50, circle, draw=black] (D) at (4,0) {$D$};
            \node[fill=cyan!50, circle, draw=black] (L) at (4,-3) {$L$};

            \path[every node/.style={font=\sffamily\scriptsize}]
            (0) edge [in=175,out=115, loop] node[above=3pt] {$(1-p(x)) \pi(a_0)$} (0)
            (0) edge [in=65,out=5, loop] node[above=3pt] {$(1-p(x)) \pi(w_0)$} (0)
            (0) edge [in=120,out=180] node[above, rotate=62] {$p(x) \pi(a_0)$} (1)
            (0) edge [bend right] node[below, rotate=62] {$p(x) \pi(w_0)$} (1)
            (1) edge [bend right] node[above, rotate=62] {$\pi(a_1)$} (0)
            (1) edge [bend right] node[rotate=18] {$\pi(w_1)$} (2)
            (2) edge [bend left] node[above, rotate=-42] {$\pi(w_2)$} (0)
            (2) edge [bend right, black!50!green] node[above, rotate=-42] {$\pi(a_2)$} (0)
            (0) edge [dotted] node[above] {$\Omega_L$} (D)
            (1) edge [dotted] node[below] {$\Omega_L$} (L)
            (2) edge [dotted] node[above, rotate=-38] {$\Omega_L$} (L);

        \end{tikzpicture}
    }
    \caption{\label{fig:obs_L_mdp} The process of reducing the delayed action task to the observable light states. One can see that $S_1$ and $S_2$ produce the same observation.}
\end{figure}

If the agent can only distinguish between light and dark states based on observations of the light, it can not distinguish $S_1$ and $S_2$.
Yet, due to the temporal structure of the MDP, one can make a difference between $S_1$ and $S_2$ as soon as the prior state of the light is also known.

This motivates the idea to use the measures shown in the previous sections, since they should be able to find this temporal correlation.
We will thus analyse reduced sequences of the original MDP and try to quantify the information encoded in them with higher block lengths.

In order to obtain these reduced sequences, we have to first generate a sequence from the true MDP and then reduce the symbols to the partial observation we want to look at (i.e. Fig. \ref{fig:obs_L_mdp} for observing the light).
Two main sources of information in this process are the state of the light ($L,D \to$ light, dark) and the actions taken by the agent ($A,W \to$ act, wait), which is why we will analyse the behaviour of both of them in the following sections.


\section{Results} \label{sec:results}

% reduced markov processes of observations
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[->,shorten >=1pt,auto,node distance=3cm,
            thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

        \node[fill=gray!50, circle, draw=black] (D) at (4,0) {$D$};
        \node[fill=cyan!50, circle, draw=black] (L) at (4,-2.5) {$L$};
        \node[fill=gray!50, circle, draw=black] (A) at (8,0) {$A$};
        \node[fill=gray!50, circle, draw=black] (W) at (8,-2.5) {$W$};

        \path[every node/.style={font=\sffamily\scriptsize}]
        (D) edge [in=110,out=70, loop] node[above] {$1-p(x)$} (D)
        (D) edge [bend right] node[above, rotate=90] {$p(x)$} (L)
        (L) edge [bend right] node[below, rotate=90] {$\mu(S_1)\pi(a_1) + \mu(S_2)$} (D)
        (L) edge [in=290,out=250, loop] node[below] {$\mu(S_1)\pi(w_1)$} (L)
        (A) edge [in=110,out=70, loop] node[above] {$P(\pi_a)$} (A)
        (A) edge [bend right] node[above, rotate=90] {$P(\pi_w)$} (W)
        (W) edge [bend right] node[below, rotate=90] {$P(\pi_a)$} (A)
        (W) edge [in=290,out=250, loop] node[below] {$P(\pi_w)$} (W);

    \end{tikzpicture}
    \caption{\label{fig:reduced_mps} Reduced models for Observation of light (left) and action (right).}
\end{figure}


\begin{figure}[H]
    \centering
    % L=2 light process
    \begin{tikzpicture}[->,shorten >=1pt,auto,node distance=3cm,
            thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

        \node[fill=gray!50, circle, draw=black] (DD) at (0,3.5) {$DD$};
        \node[fill=gray!50, circle, draw=black] (LD) at (3.5,3.5) {$LD$};
        \node[fill=cyan!50, circle, draw=black] (DL) at (0,0) {$DL$};
        \node[fill=cyan!50, circle, draw=black] (LL) at (3.5,0) {$LL$};

        \path[every node/.style={font=\sffamily\scriptsize}]
        (DD) edge [in=175,out=115, loop] node[right=1cm] {$1-p(x)$} (DD)
        (DD) edge node[left] {$p(x)$} (DL)
        (DL) edge [bend right=10]  node[below, rotate=45] {$\pi(a_1)$} (LD)
        (LD) edge [bend right=10]  node[above, rotate=45] {$p(x)$} (DL)
        (LD) edge node[above] {$1-p(x)$} (DD)
        (DL) edge node[below] {$\pi(w_1)$} (LL)
        (LL) edge node[right] {$1$} (LD);

    \end{tikzpicture}
    \caption{\label{fig:mp1_L2} Internal model with sufficient complexity.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{../figures/mp1_vs_obsL.png}
    \caption{\label{fig:mp1_vs_obsL} Plot showing the difference between MP1 and the MDP.}
\end{figure}

Talk about behaviour correlating to order 2 of process, in agreement with Crutchfield.
Also do the same plot for MP2 and action as well, showing hidden Markov convergence.

\newpage
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../figures/block_entropy_estimation.png}
    \caption{\label{fig:entropy_est} Plot showing the bias and variance of the plug in estimator for block Entropy.}
\end{figure}

$$\hat{H}(s^L) = -\sum_i \frac{n_i}{N} \log_2{\frac{n_i}{N}}$$

(Lesne et al.) (10.1103/PhysRevE.79.046208) Entropy estimation of short (time correlated) sequences
Upper bound on block length given a sequence of observations, in order to have good estimates:
$$ n \leq \frac{N h_\mu}{\ln(k)} $$

with word length $n$, length of observation sequence $N$ and number of symbols $k$.

They propose first estimating the entropy rate using Lempel-Ziv complexity(iterative algorithm moving through sequence), then setting the sequence length/word length accordingly.

$$ \hat{L}_0 = \frac{\mathcal{N}_w \ln(N)}{N},\ \hat{L}=\frac{\mathcal{N}_w[1+\log_k\mathcal{N}_w]}{N},\ \lim_{n \to \infty}\hat{L} = \frac{h_\mu}{\ln(k)} $$

With $N$ the length of the observation sequence and $\mathcal{N}_w$ parsed words from the Lempel-Ziv algorithm.
For the plug in estimator, error bars are computable, meaning computable confidence intervals.

Larson et al. (10.1016/j.procs.2011.04.172) use
$$ \mathcal{L} h_\mu \geq L|A|^L \ln|A| $$
solving for $L$ given $\mathcal{L}$ returns $W(\mathcal{L} h_\mu) / \ln|A|$ (mathematica), with the Lambert $W$ function.
(gives better estimate)

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{../figures/predictability_gain_estimation.png}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{../figures/predictability_gain_estimation_optimal.png}
\end{figure}

deviation from analytical:
[-0.00365034217876947 -0.00365907607011157 -0.0124851947847702
-0.0251811425394552]

\section{Conclusion} \label{sec:conclusion}


\section{Acknowledgements} \label{sec:acknowledgements}


\clearpage
\section*{Declaration of Authorship}

I hereby solemnly declare, by my own signature, that I have independently authored the presented work and have not used any sources or aids other than those indicated. All passages taken verbatim or in content from the specified sources are identified as such.

I consent to the archiving of this Bachelor thesis.

\hfill
\vspace{2cm} Innsbruck, \findate \hfill Lukas Prader \includegraphics[height = 10mm]{"signature.png"}


\newpage
\printbibliography[]
\input{appendix}

\end{document}